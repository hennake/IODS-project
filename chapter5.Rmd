# Exercise 5: Dimensionality reduction techniques


## Description of the data

The data used in this exercise is taken from the Human Development Report 2015 (see more info [here](http://hdr.undp.org/en/content/human-development-index-hdi)).

Two different datasets, one on human development and another on gender inequality, were combined in the data wrangling exercise to include the following variables:

* *edu2FMrat*: ratio of proportions of females/males with at least secondary education
* *labFMrat*: ratio of proportions of females/males in labour force
* *expedu*: expected years of schooling
* *lifexxp*: life expectancy at birth
* *GNI*: gross national income per capita
* *matmor*: maternal mortality ratio
* *adbi*: adolescent birth rate
* *repparl*: percentage of female representatives in parliament

Observations (countries) with missing values were removed, and the manipulated data consist of 155 countries.

Let's load the data.
```{r warning=FALSE, error=FALSE, message=FALSE}
# Load human data
library(dplyr)
library(corrplot)
library(tidyr)
load("./data/human.RData")
```


## Explorative analyses

How does the data look like? Let's produce summaries and a graphical overview of the variables:
```{r warning=FALSE, error=FALSE, message=FALSE, fig.width=10, fig.height=8}
# Structure and dimensions
str(human4)


# Variable summaries and graphical overview
summary(human4)
library(GGally)
library(ggplot2)
p <- ggpairs(human4, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```

All the variables are numerical, but have varying scales, eg. *edu2FMrat* and *labFMrat* are ratios, *repparl* is a percentage, and *GNI* is a sum index. There are several strong correlations between variables, eg. a positive correlation of 0.76 beween *adbi* (adult birth rate) and *lifeexp* (life expectancy at birth), and a negative correlation of -0.86 between *matmor* (maternal mortality ratio) and *lifeexp*.


## Principal Component Analysis (PCA)

Principal component analysis (PCA) is a dimension reduction technique that seeks to transform the data to principal components that are linear combinations of original variables. Let's first try PCA on unstandardized data.
```{r warning=FALSE, error=FALSE, message=FALSE, fig.width=6, fig.height=6}
# PCA on unstandardized data
pca_human <- prcomp(human4)
pca_human
summary(pca_human)
biplot(pca_human, choices = 1:2, cex = c(0.6, 1), col = c("grey40", "deeppink2"))
```

Doesn't look very good, as there only seems to be one important variable.This is because PCA assumes that a large variance means an important variable. The variable GNI (gross national income per capita) has a variance much larger than the other variables, and thus gets way too much weight in PCA. We need to standardize the data to scale down the effect of different-sized variances, and redo PCA.
```{r warning=FALSE, error=FALSE, message=FALSE, fig.width=6, fig.height=6}
# Standardize and repeat
human_std <- scale(human4)
summary(human_std)
pca_human_std <- prcomp(human_std)
pca_human_std
summary(pca_human_std)
biplot(pca_human_std, choices = 1:2, cex = c(0.6, 1), col = c("grey40", "darkviolet"))
```

The standardization helped, and now the results look more reasonable. The first principal component explains 53 % of variation, and the first three principal components cumulatively explain almost 80 % of the variation in our data. So we have effectively reduced the number of variables needed to characterize the data set. Moreover, the principal components are always orthogonal (ie. don't correlate with each other), which is handy if we would like to perform further analyses, e.g. linear regression, on them.

The first principal component (PC1) is a linear combination of other variables than female representation in labour force and in parliament (*labFMrat* and *repparl*, respectively), as these two variables are the only ones to have low loadings on PC1 (ie. don't correlate with PC1). On the other hand, these two variables have high loadings on PC2, whereas other variables barely correlate with PC2. Female representation both in labour force and parliament positively correlates with PC2, so we can interpret PC2 to represent gender equality (most gender-equal countries are located on top of the biplot). Female participation in secondary education (*edu2FMrat*), life expectation (*lifeexp*), expected years of education (*expedu*) and gross national income per capita (*GNI*) have negative loadings on PC1, whereas maternal mortality rate (*matmor*) and adolescent birth rate (*adbi*) have positive loadings on it. This seems to imply that high value of PC1 (location on right of the biplot) signifies low development level of a country, combining social, educational and economic aspects of development.


## Multiple Correspondence Analysis (MCA)

Next, we try another dimension reduction method on tea data from the package FactoMineR. This dataset includes 300 observations of people's tea drinking habits.
```{r warning=FALSE, error=FALSE, message=FALSE}
# Load tea dataset
library(FactoMineR)
data(tea)
str(tea)
```

The dataset includes almost exclusively factor variables, so we can't now perform PCA, which requires numerical variables. There is a very high number of variables in the data, so let's only choose a subset of them for further analysis.
```{r warning=FALSE, error=FALSE, message=FALSE, fig.width=7, fig.height=7}
library(tidyr)
tea2 <- tea[c("breakfast","tea.time","friends","Tea","How","sugar","sex","sophisticated")]
str(tea2)
gather(tea2) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
```

Let's perform a multiple correspondence analysis (MCA) on these 8 variables. MCA is a generalization of PCA that can be used to visualize categorical data on Euclidean space.
```{r warning=FALSE, error=FALSE, message=FALSE}
# MCA
res.mca <- MCA(tea2, graph=FALSE)
summary(res.mca, nbelements=Inf, nbind=5)
```

The first two dimensions retain 27 % of variance in the data. The v-test values show that most categories have significant (abs(v.test) > 1.96) coordinates in the first three dimensions. Variables sex and tea time have highest correlations with the dimension 1, and variables breakfast, Tea and How have highest correlations with the dimension 2.

We can visualize the results with biplots using different options.
```{r warning=FALSE, error=FALSE, message=FALSE, fig.width=6, fig.height=6}
# MCA
plot(res.mca, invisible = c("var"))
plot(res.mca, invisible = c("ind"), habillage="quali")
```

The second biplot shows the individual categories plotted on (Euclidean) MCA space formed by first two MCA dimensions. Categories that are mutually similar are positioned close to each other in the biplot. We can see one clear outlier category, those who choose to drink their tea with "other" ways, on top of the plot. However, there are very few cases in this category, as could be seen on the barplot that was plotted earlier. 

In the second biplot, dimension 1 separates females and males to different groups, as the category summary above implied. Females seem to prefer to drink their tea at tea time, with friends and with no sugar, whereas men are not that particular about drinking tea at tea time or with friends, and more often use sugar. Dimension 2 separates those drinking their tea on breakfast and those who do not drink tea on breakfast. The former group prefers drinking black tea with milk, whereas the latter chooses green tea or Earl Grey (weird! isn't Earl Grey a black tea variant?) alone or with lemon.


